import sys
from pathlib import Path
import time
from typing import Any, Dict, List, Optional

import streamlit as st
import pandas as pd
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import json

# ç¡®ä¿å¯ä»¥å¯¼å…¥ datasets_v1.datasets_summary
WORKSPACE_ROOT = Path(__file__).resolve().parent
DATASETS_DIR = WORKSPACE_ROOT / "datasets_v1"
if str(WORKSPACE_ROOT) not in sys.path:
    sys.path.append(str(WORKSPACE_ROOT))

DATASETS_JSON_PATH = WORKSPACE_ROOT / "datasets_v1/datasets_summary.json"

def load_dataset(dataset_path):
    try:
        with open(dataset_path, "r", encoding="utf-8") as f:
            # æ›´æ–°
            st.session_state["datasets_summary"] = json.load(f)
    except Exception as import_error:  # pragma: no cover
        st.error(f"æ— æ³•è¯»å–æ•°æ®é›†æ‘˜è¦ï¼š{import_error}")
        st.stop()


def to_dataset_rows(summary: List[Dict[str, Any]]) -> pd.DataFrame:
    rows: List[Dict[str, Any]] = []
    for ds in summary:
        dataset_name = ds.get("dataset_name")
        update_time = ds.get("update_time")
        try:
            update_dt = datetime.strptime(update_time, "%Y-%m-%d") if update_time else None
        except Exception:
            update_dt = None
        rows.append(
            {
                "dataset_name": dataset_name,
                "dataset_version": ds.get("dataset_version"),
                "description": ds.get("description"),
                "paper_url": ds.get("paper_url"),
                "dataset_url": ds.get("dataset_url"),
                "github_url": ds.get("github_url"),
                "languages": ds.get("languages", []),
                "task_type": ds.get("task_type"),
                "tool_preference": ds.get("tool_preference", []),
                "num_total_samples": ds.get("num_total_samples", 0),
                "dataset_splits": ds.get("dataset_splits", []),
                "update_time": update_time,
                "update_dt": update_dt,
                "organization": ds.get("organization"),
            }
        )
    return pd.DataFrame(rows)


def to_split_rows(summary: List[Dict[str, Any]]) -> pd.DataFrame:
    # æ£€æŸ¥å‚æ•°æ˜¯å¦é—æ¼ï¼Œå‚è€ƒ datasets_summary.py çš„ split å­—æ®µ
    rows: List[Dict[str, Any]] = []
    for ds in summary:
        dataset_name = ds.get("dataset_name")
        base_info = {
            "dataset_name": dataset_name,
            "dataset_version": ds.get("dataset_version"),
            "languages": ds.get("languages", []),
            "task_type": ds.get("task_type"),
            "tool_preference": ds.get("tool_preference", []),
            "organization": ds.get("organization"),
            "update_time": ds.get("update_time"),
        }
        for sp in ds.get("dataset_splits", []) or []:
            status = (sp.get("status") or {}).get("run_progess") or {}
            processed = status.get("processed_samples", 0) or 0
            correct = status.get("correct_samples", 0) or 0
            num_samples = sp.get("num_samples", 0) or 0

            # æ£€æŸ¥ split å­—æ®µï¼Œè¡¥å……é—æ¼å­—æ®µ
            rows.append(
                {
                    **base_info,
                    "split_name": sp.get("split_name"),
                    "display_name": sp.get("display_name"),
                    "num_samples": num_samples,
                    "file_path": sp.get("file_path"),
                    "has_reference_answer": sp.get("has_reference_answer", False),
                    "has_reasoning_process": sp.get("has_reasoning_process", False),
                    "is_multimodal": sp.get("is_multimodal", False),
                    "originality": sp.get("originality", []),
                    "column_mapping": sp.get("column_mapping", {}),
                    "split_description": sp.get("description", "æ— æè¿°"),
                    "processed_samples": processed,
                    "correct_samples": correct,
                    "correct_samples_path": status.get("correct_samples_path"),
                    "status_downloaded": (sp.get("status") or {}).get("downloaded", False),
                }
            )
    return pd.DataFrame(rows)


def get_unique_values(df: pd.DataFrame, column: str) -> List[Any]:
    values: List[Any] = []
    if column not in df.columns:
        return values
    series = df[column].dropna()
    if series.empty:
        return values
    if series.apply(lambda x: isinstance(x, list)).any():
        all_values: List[Any] = []
        for item in series:
            if isinstance(item, list):
                all_values.extend(item)
        values = sorted(pd.unique(pd.Series(all_values)).tolist())
    else:
        values = sorted(series.astype(str).unique().tolist())
    return values


def apply_filters(
    df: pd.DataFrame,
    languages: List[str],
    task_types: List[str],
    tools: List[str],
    organizations: List[str],
    originality: List[str],
    is_multimodal: Optional[bool],
    has_reference_answer: Optional[bool],
    has_reasoning_process: Optional[bool],
) -> pd.DataFrame:
    filtered = df.copy()

    if languages:
        filtered = filtered[filtered["languages"].apply(lambda lst: any(lang in (lst or []) for lang in languages))]

    if task_types:
        filtered = filtered[filtered["task_type"].astype(str).isin(task_types)]

    if tools:
        filtered = filtered[filtered["tool_preference"].apply(lambda lst: any(t in (lst or []) for t in tools))]

    if organizations:
        filtered = filtered[filtered["organization"].astype(str).isin(organizations)]

    if originality and "originality" in filtered.columns:
        filtered = filtered[filtered["originality"].apply(lambda lst: any(o in (lst or []) for o in originality))]

    def maybe_filter_bool(column: str, value: Optional[bool]) -> None:
        nonlocal filtered
        if value is not None and column in filtered.columns:
            filtered = filtered[filtered[column] == value]

    maybe_filter_bool("is_multimodal", is_multimodal)
    maybe_filter_bool("has_reference_answer", has_reference_answer)
    maybe_filter_bool("has_reasoning_process", has_reasoning_process)

    return filtered


def render_metrics(datasets_df: pd.DataFrame, splits_df: pd.DataFrame) -> None:
    # æŒ‡æ ‡å¡ç‰‡æ ·å¼ï¼ˆç¼©å°å°ºå¯¸ï¼‰
    card_style = """
    <style>
    .kpi-card {
        background: #f8f9fa;
        border-radius: 10px;
        padding: 8px 0 4px 0;
        margin-bottom: 0px;
        box-shadow: 0 1px 4px 0 rgba(0,0,0,0.04);
        text-align: center;
        height: 60px;
        min-height: 60px;
        max-height: 60px;
        display: flex;
        flex-direction: column;
        justify-content: center;
    }
    .kpi-title {
        font-size: 13px;
        color: #666;
        margin-bottom: 2px;
        font-weight: 500;
    }
    .kpi-value {
        font-size: 22px;
        font-weight: bold;
        color: #222;
        margin-bottom: 0px;
    }
    .kpi-trend {
        font-size: 12px;
        font-weight: 500;
        margin-top: 2px;
        display: flex;
        align-items: center;
        justify-content: center;
    }
    .kpi-trend-up {
        color: #27ae60;
    }
    .kpi-trend-down {
        color: #e74c3c;
    }
    .kpi-trend-flat {
        color: #888;
    }
    </style>
    """

    st.markdown(card_style, unsafe_allow_html=True)
    col1, col2, col3, col4 = st.columns([2, 2, 2, 2])
    with col1:
        st.markdown(
            f"""
            <div class="kpi-card">
                <div class="kpi-title">ğŸ“š æ•°æ®é›†æ•°</div>
                <div class="kpi-value">{int(datasets_df.shape[0])}</div>
            </div>
            """, unsafe_allow_html=True
        )
    with col2:
        st.markdown(
            f"""
            <div class="kpi-card">
                <div class="kpi-title">ğŸ—‚ï¸ åˆ‡åˆ†(Split)æ•°</div>
                <div class="kpi-value">{int(splits_df.shape[0])}</div>
            </div>
            """, unsafe_allow_html=True
        )
    with col3:
        total_samples = int(datasets_df["num_total_samples"].fillna(0).sum()) if not datasets_df.empty else 0
        st.markdown(
            f"""
            <div class="kpi-card">
                <div class="kpi-title">ğŸ”¢ æ ·æœ¬æ€»é‡</div>
                <div class="kpi-value">{total_samples:,}</div>
            </div>
            """, unsafe_allow_html=True
        )
    with col4:
        # è¯­è¨€åˆ†å¸ƒé¥¼å›¾
        lang_set = set()
        lg_map = {
            "zh": "ä¸­",
            "en": "è‹±",
            "ja": "æ—¥",
            "lat": "æ‹‰ä¸",
        }
        for _, row in datasets_df.iterrows():
            langs = row.get("languages") or []
            for lg in langs:
                lang_set.add(lg)
        lang_list = sorted(list(lang_set))
        if lang_list:
            text = []
            for lg in lang_list:
                text.append(f"âœ… {lg_map[lg]}  ")
            text_str = ' '.join(text)
            st.markdown(f"""
            <div class="kpi-card">
                <div class="kpi-title">ğŸ’¬ è¯­è¨€æ”¯æŒ</div>
                <div class="kpi-value">{len(text)}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("å½“å‰ç­›é€‰æ— è¯­è¨€ä¿¡æ¯")


def render_dataset_charts(datasets_df: pd.DataFrame, splits_df: pd.DataFrame, animate: bool = True) -> None:
    if datasets_df.empty:
        st.info("æ— æ•°æ®ç”¨äºå¯è§†åŒ–")
        return
    st.caption("")
    st.subheader("ğŸ“Š æ ·æœ¬é‡", help="ğŸ‹ æ ·æœ¬æ•°é‡åŸºäº QAå¯¹ æ•°æ®é‡è¿›è¡Œç»Ÿè®¡")
    st.caption("æ³¨æ„ï¼šy è½´æ”¹ä¸º**å¯¹æ•°åˆ»åº¦**ï¼Œè®©å‡ ç™¾å’Œä¸€ä¸‡éƒ½èƒ½åœ¨å›¾é‡Œæ¸…æ™°å¯è§")
    # æ•°æ®é›†åå’Œåˆ‡åˆ†åç¼©å†™å¤„ç†
    def short_name(name, max_len=15):
        name = str(name)
        return name if len(name) <= max_len else name[:max_len] + "..."

    def short_split_name(name, max_len=15):
        name = str(name)
        return name if len(name) <= max_len else name[:max_len] + "..."

    datasets_df = datasets_df.copy()
    datasets_df["dataset_name_short"] = datasets_df["dataset_name"].apply(short_name)

    # æŒ‰æ•°æ®é›†ç»˜åˆ¶æ ·æœ¬é‡æŸ±çŠ¶å›¾ï¼Œyè½´ä½¿ç”¨å¯¹æ•°åæ ‡
    fig1 = px.bar(
        datasets_df.sort_values("num_total_samples", ascending=False),
        x="dataset_name_short",
        y="num_total_samples",
        color="organization",
        labels={"dataset_name_short": "æ•°æ®é›†", "num_total_samples": "æ ·æœ¬é‡", "organization": "ç»„ç»‡"},
        hover_data=["dataset_name", "organization", "num_total_samples"],
        height=320,
    )
    fig1.update_layout(
        xaxis_title="æ•°æ®é›†",
        yaxis_title="æ ·æœ¬é‡",
        legend_title="ç»„ç»‡",
        margin=dict(l=20, r=20, t=30, b=20),
        plot_bgcolor="#fff",
        paper_bgcolor="#fff",
        font=dict(size=14),
        yaxis_type="log",  # è®¾ç½®yè½´ä¸ºå¯¹æ•°åæ ‡
        transition={"duration": 500, "easing": "cubic-in-out"} if animate else None,
    )
    st.plotly_chart(fig1, width='stretch', config={"displayModeBar": False})

    st.subheader("ğŸ“¦ åˆ‡åˆ†(Split)æ ·æœ¬é‡", help="ğŸ‹ æ ·æœ¬æ•°é‡åŸºäº QAå¯¹ æ•°æ®é‡è¿›è¡Œç»Ÿè®¡")
    st.caption("æ³¨æ„ï¼šy è½´æ”¹ä¸º**å¯¹æ•°åˆ»åº¦**ï¼Œè®©å‡ ç™¾å’Œä¸€ä¸‡éƒ½èƒ½åœ¨å›¾é‡Œæ¸…æ™°å¯è§")
    if not splits_df.empty:
        # å †å æŸ±çŠ¶å›¾
        agg = splits_df.groupby(["dataset_name", "display_name"], as_index=False)["num_samples"].sum()
        agg = agg.copy()
        agg["dataset_name_short"] = agg["dataset_name"].apply(short_name)
        agg["display_name_short"] = agg["display_name"].apply(short_split_name)
        fig2 = px.bar(
            agg,
            x="dataset_name_short",
            y="num_samples",
            color="display_name_short",
            labels={"dataset_name_short": "æ•°æ®é›†", "num_total_samples": "æ ·æœ¬é‡", "display_name_short": "åˆ‡åˆ†"},
            height=320,
            hover_data=["dataset_name", "display_name", "num_samples"],  # å¢åŠ  hover data æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
        )
        # å–æ¶ˆ legend çš„è‡ªå®šä¹‰ï¼Œä½¿ç”¨é»˜è®¤
        fig2.update_layout(
            barmode="stack",
            xaxis_title="æ•°æ®é›†",
            yaxis_title="æ ·æœ¬é‡",
            margin=dict(l=20, r=20, t=30, b=20),
            plot_bgcolor="#fff",
            paper_bgcolor="#fff",
            font=dict(size=14),
            yaxis_type="log",  # è®¾ç½®yè½´ä¸ºå¯¹æ•°åæ ‡
            transition={"duration": 500, "easing": "cubic-in-out"} if animate else None,
        )
        st.plotly_chart(fig2, width='stretch', config={"displayModeBar": False})

        # æ–°å¢ï¼šæŒ‰ tool_preference åˆ†ç±»çš„é¥¼çŠ¶å›¾
        st.subheader("ğŸ› ï¸ å·¥å…·åå¥½åˆ†å¸ƒ", help="âš ï¸ ç²—ç•¥ç»Ÿè®¡ï¼ŒåŒä¸€æ¡æ•°æ®å¯èƒ½ä¼šåœ¨å¤šä¸ªå·¥å…·åå¥½ä¸­å‡ºç°")
        import itertools
        import pandas as pd

        if "tool_preference" in datasets_df.columns:
            tool_rows = []
            for _, row in datasets_df.iterrows():
                tps = row.get("tool_preference", [])
                if not isinstance(tps, list):
                    continue
                for tp in tps:
                    tool_rows.append({
                        "dataset_name": row["dataset_name"],
                        "tool_preference": tp,
                        "num_total_samples": row.get("num_total_samples", 0)
                    })
            if tool_rows:
                tool_df = pd.DataFrame(tool_rows)
                # æŒ‰ tool_preference åˆ†ç»„ç»Ÿè®¡æ ·æœ¬æ€»é‡
                agg_tool = tool_df.groupby("tool_preference", as_index=False)["num_total_samples"].sum()
                agg_tool["percent"] = agg_tool["num_total_samples"] / agg_tool["num_total_samples"].sum() * 100

                # æ ‡è®°å æ¯”å°äº1%çš„å·¥å…·
                agg_tool["tool_preference_display"] = agg_tool.apply(
                    lambda row: f"{row['tool_preference']} ğŸ”" if row["percent"] < 1 else row["tool_preference"], axis=1
                )

                fig4 = px.pie(
                    agg_tool,
                    names="tool_preference_display",
                    values="num_total_samples",
                    color="tool_preference_display",
                    labels={"tool_preference_display": "å·¥å…·", "num_total_samples": "æ ·æœ¬é‡"},
                    height=320,
                    hole=0.3,
                    hover_data=["tool_preference", "num_total_samples", "percent"]
                )
                fig4.update_traces(
                    textinfo='percent+label',
                    hovertemplate="<b>%{label}</b><br>æ ·æœ¬é‡: %{value}<br>å æ¯”: %{percent:.2%}<extra></extra>"
                )

                # é¥¼å›¾ä¸Šåªæ˜¾ç¤ºå¤§äº1%çš„æ ‡ç­¾ï¼Œå…¶ä½™åªåœ¨æ‚¬æµ®æ—¶æ˜¾ç¤º
                fig4.update_traces(
                    texttemplate=[
                        f"{row['tool_preference']}: {row['percent']:.1f}%" if row["percent"] >= 1 else ""
                        for _, row in agg_tool.iterrows()
                    ]
                )

                fig4.update_layout(
                    margin=dict(l=20, r=20, t=30, b=20),
                    plot_bgcolor="#fff",
                    paper_bgcolor="#fff",
                    font=dict(size=14),
                    transition={"duration": 500, "easing": "cubic-in-out"} if animate else None,
                )
                st.plotly_chart(fig4, width='stretch', config={"displayModeBar": False})

                # é¥¼å›¾ä¸‹æ–¹å¢åŠ è¯´æ˜
                if (agg_tool["percent"] < 1).any():
                    st.caption(
                        """
                        <div style="display: flex; align-items: center; gap: 8px;">
                            <span style="font-size: 1.5em;">ğŸ”</span>
                            <span style="font-size: 1.1em;">éƒ¨åˆ†å·¥å…·å æ¯”ä½äº 1%ï¼Œä»…åœ¨æ‚¬æµ®æ—¶æ˜¾ç¤ºã€‚</span>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
            else:
                st.info("å½“å‰ç­›é€‰æ— å·¥å…·åå¥½ä¿¡æ¯")
        else:
            st.info("å½“å‰ç­›é€‰æ— å·¥å…·åå¥½ä¿¡æ¯")


def render_split_charts(splits_df: pd.DataFrame, animate: bool = True) -> None:
    if splits_df.empty:
        st.info("æ— åˆ‡åˆ†æ•°æ®ç”¨äºå¯è§†åŒ–")
        return
    st.caption("")
    st.subheader("ğŸ“Š æ ·æœ¬é‡", help="ğŸ‹ æ ·æœ¬æ•°é‡åŸºäº QAå¯¹ æ•°æ®é‡è¿›è¡Œç»Ÿè®¡")
    st.caption("æ³¨æ„ï¼šy è½´æ”¹ä¸º**å¯¹æ•°åˆ»åº¦**ï¼Œè®©å‡ ç™¾å’Œä¸€ä¸‡éƒ½èƒ½åœ¨å›¾é‡Œæ¸…æ™°å¯è§")
    # display_name åªæ˜¾ç¤ºå‰15ä¸ªå­—ç¬¦ï¼Œè¶…å‡ºéƒ¨åˆ†ç”¨...ä»£æ›¿
    agg = splits_df.groupby(["display_name", "dataset_name"], as_index=False)["num_samples"].sum()
    agg["display_name_short"] = agg["display_name"].apply(lambda x: x[:15] + "..." if len(str(x)) > 10 else x)
    fig = px.bar(
        agg.sort_values("num_samples", ascending=False),
        x="display_name_short",
        y="num_samples",
        color="dataset_name",
        labels={"display_name_short": "åˆ‡åˆ†", "num_samples": "æ ·æœ¬é‡", "dataset_name": "æ•°æ®é›†"},
        hover_data=["dataset_name", "display_name"],
        height=320,
    )
    fig.update_layout(
        xaxis_title="åˆ‡åˆ†",
        yaxis_title="æ ·æœ¬é‡",
        legend_title="æ•°æ®é›†",
        margin=dict(l=20, r=20, t=30, b=20),
        plot_bgcolor="#fff",
        paper_bgcolor="#fff",
        font=dict(size=14),
        yaxis_type="log",  # è®¾ç½®yè½´ä¸ºå¯¹æ•°åæ ‡
        transition={"duration": 500, "easing": "cubic-in-out"} if animate else None,
    )
    st.plotly_chart(fig, width='stretch', config={"displayModeBar": False})


def render_progress(splits_df: pd.DataFrame) -> None:
    st.subheader("ğŸš€ è¿è¡Œè¿›åº¦")
    if splits_df.empty:
        st.info("æ— åˆ‡åˆ†è¿›åº¦å¯æ˜¾ç¤º")
        return

    for _, row in splits_df.iterrows():
        ds = row["dataset_name"]
        sp = row["display_name"] or row.get("split_name")
        num_samples = int(row.get("num_samples", 0) or 0)
        processed = int(row.get("processed_samples", 0) or 0)
        correct = int(row.get("correct_samples", 0) or 0)
        log_path = row.get("correct_samples_path")
        progress = min(processed / num_samples, 1.0) if num_samples > 0 else 0
        acc = min(correct / processed, 1.0) if processed > 0 else 0

        st.markdown(f"**{ds} / {sp}**")
        col1, col2 = st.columns([2, 8])
        with col1:
            st.write(f"å·²å¤„ç† {processed:,} / {num_samples:,}ï¼ˆ{progress*100:.2f}%ï¼‰")
        with col2:
            st.progress(progress, text="å¤„ç†è¿›åº¦")
        col3, col4 = st.columns([2, 8])
        with col3:
            st.write(f"æ­£ç¡® {correct:,} / å·²å¤„ç† {processed:,}ï¼ˆ{acc*100:.2f}%ï¼‰")
        with col4:
            st.progress(acc, text="æ­£ç¡®ç‡")
        if isinstance(log_path, str) and log_path:
            st.caption(f"æ­£ç¡®æ ·æœ¬è·¯å¾„ï¼š{log_path}")
        st.divider()

@st.dialog("åˆ é™¤æ•°æ®é›†", width="medium")
def delete_datasets_dialog():
    # è·å–æ‰€æœ‰æ•°æ®é›†åç§°
    ds_list = [ds.get("dataset_name", "") for ds in st.session_state.get("datasets_summary", [])]
    if not ds_list:
        st.info("å½“å‰æ²¡æœ‰å¯åˆ é™¤çš„æ•°æ®é›†ã€‚")
        return

    # ä½¿ç”¨checkboxé€‰æ‹©è¦åˆ é™¤çš„æ•°æ®é›†
    selected_to_delete = []
    with st.container(height=300):
        for ds_name in ds_list:
            if st.checkbox(f"åˆ é™¤ï¼š{ds_name}", key=f"delete_ds_checkbox_{ds_name}"):
                selected_to_delete.append(ds_name)
    confirm_text = st.text_input("è¯·è¾“å…¥â€œæˆ‘ç¡®å®šåˆ é™¤â€ä»¥ç¡®è®¤åˆ é™¤æ“ä½œ", key="delete_ds_confirm")

    if st.button("åˆ é™¤é€‰ä¸­æ•°æ®é›†", key="delete_ds_btn"):
        if not selected_to_delete:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¦åˆ é™¤çš„æ•°æ®é›†ã€‚")
            return
        if confirm_text != "æˆ‘ç¡®å®šåˆ é™¤":
            st.error("è¯·æ­£ç¡®è¾“å…¥â€œæˆ‘ç¡®å®šåˆ é™¤â€ä»¥ç¡®è®¤åˆ é™¤ã€‚")
            return

        # æ‰§è¡Œåˆ é™¤
        before_count = len(st.session_state["datasets_summary"])
        st.session_state["datasets_summary"] = [
            ds for ds in st.session_state["datasets_summary"]
            if ds.get("dataset_name", "") not in selected_to_delete
        ]
        after_count = len(st.session_state["datasets_summary"])
        try:
            with open(DATASETS_JSON_PATH, "w", encoding="utf-8") as f:
                import json
                json.dump(st.session_state["datasets_summary"], f, ensure_ascii=False, indent=4)
            st.toast(f"æˆåŠŸåˆ é™¤ {before_count - after_count} ä¸ªæ•°æ®é›†ï¼Œå·²å†™å…¥ï¼", icon="ğŸ—‘ï¸")
            time.sleep(1)
        except Exception as e:
            st.error(f"å†™å…¥æœ¬åœ°æ–‡ä»¶å¤±è´¥ï¼š{e}")

        time.sleep(1)
        st.rerun()


@st.dialog("æ–°å¢æ•°æ®é›†", width="medium")
def create_new_dataset(lang_opts, task_opts, tool_opts, org_opts, originality_opts):
    st.subheader("å¡«å†™æ•°æ®é›†ä¿¡æ¯")
    
    splits = st.session_state["add_ds_splits"]

    def add_split():
        splits.append({})
        st.session_state["add_ds_splits"] = splits

    def remove_split(idx):
        splits.pop(idx)
        st.session_state["add_ds_splits"] = splits

    split_cols = st.columns([2,8])
    with split_cols[0]:
        if st.button("â• æ·»åŠ åˆ‡åˆ†", key="add_split_btn"):
            add_split()
            st.rerun()
    # åˆ é™¤splitæŒ‰é’®
    with split_cols[1]:
        btn_cols = st.columns(5)
        for idx, split in enumerate(splits):
            with btn_cols[idx]:
                if st.button(f"åˆ‡åˆ†{idx+1} ğŸš«", key=f"del_split_{idx}"):
                    remove_split(idx)
                    st.rerun()
    with st.container(border= False, height= 500):
        with st.form("add_dataset_form", clear_on_submit=False):
            dataset_name = st.text_input("æ•°æ®é›†åç§°", key="add_ds_name")
            dataset_version = st.text_input("æ•°æ®é›†ç‰ˆæœ¬", key="add_ds_version")
            description = st.text_area("æ•°æ®é›†æè¿°", key="add_ds_desc")
            paper_url = st.text_input("è®ºæ–‡é“¾æ¥", key="add_ds_paper")
            dataset_url = st.text_input("æ•°æ®é›†é“¾æ¥", key="add_ds_url")
            github_url = st.text_input("GitHub é“¾æ¥", key="add_ds_github")
            # å¤šé€‰/å¯æ–°å»º
            languages = st.multiselect("è¯­è¨€", options=lang_opts, key="add_ds_lang", accept_new_options = True)
            task_type = st.selectbox("ä»»åŠ¡ç±»å‹", options=task_opts, key="add_ds_task", accept_new_options = True)
            tool_preference = st.multiselect("å·¥å…·åå¥½", options=tool_opts, key="add_ds_tool", accept_new_options = True)
            num_total_samples = st.number_input("æ€»æ ·æœ¬æ•°", min_value=0, step=1, key="add_ds_total")
            update_time = st.date_input("æ›´æ–°æ—¶é—´", key="add_ds_update")
            organization = st.selectbox("ç»„ç»‡", options=org_opts, key="add_ds_org", accept_new_options = True)
            # dataset_splits åŠ¨æ€æ·»åŠ 
            st.markdown("#### åˆ‡åˆ†ï¼ˆSplitsï¼‰")
            for idx, split in enumerate(splits):
                with st.expander(f"åˆ‡åˆ† {idx+1}", expanded=False):
                    split["split_name"] = st.text_input(f"åˆ‡åˆ†å", value=split.get("split_name", ""), key=f"split_name_{idx}")
                    split["display_name"] = st.text_input(f"æ˜¾ç¤ºå", value=split.get("display_name", ""), key=f"display_name_{idx}")
                    split["num_samples"] = st.number_input(f"æ ·æœ¬æ•°", min_value=0, step=1, value=split.get("num_samples", 0), key=f"num_samples_{idx}")
                    split["file_path"] = st.text_input(f"æ–‡ä»¶è·¯å¾„", value=split.get("file_path", ""), key=f"file_path_{idx}")
                    split["has_reference_answer"] = st.checkbox(f"æœ‰å‚è€ƒç­”æ¡ˆ", value=split.get("has_reference_answer", False), key=f"has_ref_{idx}")
                    split["has_reasoning_process"] = st.checkbox(f"æœ‰æ¨ç†è¿‡ç¨‹", value=split.get("has_reasoning_process", False), key=f"has_reason_{idx}")
                    split["is_multimodal"] = st.checkbox(f"å¤šæ¨¡æ€", value=split.get("is_multimodal", False), key=f"is_multi_{idx}")
                    split["description"] = st.text_area(f"åˆ‡åˆ†æè¿°", value=split.get("description", ""), key=f"desc_{idx}")
                    # originality å¤šé€‰
                    split["originality"] = st.multiselect(f"åŸåˆ›æ€§", options=originality_opts, default=split.get("originality", []), key=f"orig_{idx}")
                    # column_mapping å­—å…¸
                    st.markdown("å­—æ®µæ˜ å°„ï¼ˆå¯é€‰ï¼‰")
                    if "column_mapping" not in split:
                        split["column_mapping"] = {}
                    col_map = split["column_mapping"]
                    col_map["id"] = st.text_input("å­—æ®µæ˜ å°„-id", value=col_map.get("id", ""), key=f"colmap_id_{idx}")
                    col_map["question"] = st.text_input("å­—æ®µæ˜ å°„-question", value=col_map.get("question", ""), key=f"colmap_q_{idx}")
                    col_map["answer"] = st.text_input("å­—æ®µæ˜ å°„-answer", value=col_map.get("answer", ""), key=f"colmap_a_{idx}")
                    col_map["reasoning_process"] = st.text_input("å­—æ®µæ˜ å°„-reasoning_process", value=col_map.get("reasoning_process", ""), key=f"colmap_r_{idx}")
                    col_map["multimodal_file_path"] = st.text_input("å­—æ®µæ˜ å°„-multimodal_file_path", value=col_map.get("multimodal_file_path", ""), key=f"colmap_m_{idx}")
                    col_map["reference"] = st.text_input("å­—æ®µæ˜ å°„-reference", value=col_map.get("reference", ""), key=f"colmap_ref_{idx}")
                    split["column_mapping"] = col_map
                    # status å­—å…¸
                    st.markdown("çŠ¶æ€ï¼ˆå¯é€‰ï¼‰")
                    if "status" not in split:
                        split["status"] = {"downloaded": False, "run_progess": {"processed_samples": 0, "correct_samples": 0, "correct_samples_path": None}}
                    status = split["status"]
                    status["downloaded"] = st.checkbox("å·²ä¸‹è½½", value=status.get("downloaded", False), key=f"downloaded_{idx}")
                    run_progess = status.get("run_progess", {})
                    run_progess["processed_samples"] = st.number_input("å·²å¤„ç†æ ·æœ¬æ•°", min_value=0, step=1, value=run_progess.get("processed_samples", 0), key=f"proc_{idx}")
                    run_progess["correct_samples"] = st.number_input("æ­£ç¡®æ ·æœ¬æ•°", min_value=0, step=1, value=run_progess.get("correct_samples", 0), key=f"corr_{idx}")
                    run_progess["correct_samples_path"] = st.text_input("æ­£ç¡®æ ·æœ¬è·¯å¾„", value=run_progess.get("correct_samples_path", ""), key=f"corr_path_{idx}")
                    status["run_progess"] = run_progess
                    split["status"] = status
            #         # åˆ é™¤splitæŒ‰é’®
            #         if st.button(f"åˆ é™¤åˆ‡åˆ† {idx+1}", key=f"del_split_{idx}"):
            #             remove_split(idx)
            #             st.experimental_rerun()
            # if st.button("â• æ·»åŠ åˆ‡åˆ†", key="add_split_btn"):
            #     add_split()
            #     st.rerun()

            submitted = st.form_submit_button("æäº¤")
            if submitted:
                # æ„é€ æ–°æ•°æ®é›†dict
                new_ds = {
                    "dataset_name": dataset_name,
                    "dataset_version": dataset_version,
                    "description": description,
                    "paper_url": paper_url,
                    "dataset_url": dataset_url,
                    "github_url": github_url,
                    "languages": languages,
                    "task_type": task_type,
                    "tool_preference": tool_preference,
                    "num_total_samples": num_total_samples,
                    "dataset_splits": [s for s in splits if s.get("split_name", "") != ""],
                    "update_time": update_time.strftime("%Y-%m-%d") if update_time else "",
                    "organization": organization,
                }
                # æ›´æ–°æœ¬åœ°datasets_summaryå¹¶å†™å…¥æ–‡ä»¶
                st.session_state["datasets_summary"].append(new_ds)
                try:
                    with open(DATASETS_JSON_PATH, "w", encoding="utf-8") as f:
                        import json
                        json.dump(st.session_state["datasets_summary"], f, ensure_ascii=False, indent=4)
                    # with st.spinner("å¡«å†™æˆåŠŸï¼Œæ­£åœ¨å¡«å…¥..."):  
                    st.toast("æ–°å¢æ•°æ®é›†æˆåŠŸ, å·²å†™å…¥!", icon="ğŸ’¡")
                    time.sleep(1)
                except Exception as e:
                    st.error(f"å†™å…¥æœ¬åœ°æ–‡ä»¶å¤±è´¥ï¼š{e}")
                
                # æ¸…ç©ºè¡¨å•
                st.session_state["add_ds_form"] = {}
                st.session_state["add_ds_splits"] = [{},{},{}]
                time.sleep(1)
                st.rerun()


def initialze_sesstion_state():
    if "password_verified" not in st.session_state:
        st.session_state["password_verified"] = False
    # åˆå§‹åŒ–æ•°æ®é›†
    if "datasets_summary" not in st.session_state:
        st.session_state["datasets_summary"] = []
        load_dataset(DATASETS_JSON_PATH)

    if "add_ds_splits" not in st.session_state:
        st.session_state["add_ds_splits"] = [{},{},{}]

    if "add_ds_form" not in st.session_state:
        st.session_state["add_ds_form"] = {}


def render_dataset_description_card(ds_row: pd.Series) -> None:
    # ä½¿ç”¨ st.expander ç»“åˆ st.caption å±•ç¤ºæ•°æ®é›†æè¿°ï¼Œå¹¶åˆ†è¡Œæ˜¾ç¤º descriptionï¼ŒåŒæ—¶å±•ç¤ºæ¯ä¸ª split çš„ description
    dataset_name = ds_row['dataset_name']
    dataset_version = ds_row.get('dataset_version', '')
    description = ds_row.get('description', 'æ— æè¿°')
    paper_url = ds_row.get('paper_url') if len(str(ds_row.get('paper_url', ''))) else "æ— "
    dataset_url = ds_row.get('dataset_url') if len(str(ds_row.get('dataset_url', ''))) else "æ— "
    github_url = ds_row.get('github_url') if len(str(ds_row.get('github_url', ''))) else "æ— "
    splits = ds_row.get('dataset_splits', [])
    title_str = f"{dataset_name}ï¼ˆv{dataset_version}ï¼‰" if dataset_version else dataset_name

    with st.expander(f"ğŸ“ æ•°æ®é›†ç®€ä»‹ï¼š{title_str}", expanded=True):
        # åˆ†è¡Œæ˜¾ç¤º description
        for line in str(description).splitlines():
            st.caption(line if line.strip() else "ã€€")
        st.caption(f"ğŸ“„ è®ºæ–‡: {paper_url}")
        st.caption(f"ğŸ“¦ æ•°æ®é›†: {dataset_url}")
        st.caption(f"ğŸ’» GitHub: {github_url}")
        # å±•ç¤ºæ¯ä¸ª split çš„ description
        if splits and isinstance(splits, (list, tuple)):
            for idx, split in enumerate(splits):
                split_name = split.get('display_name') or split.get('split_name') or 'æœªå‘½ååˆ‡åˆ†'
                split_desc = split.get('description', 'æ— æè¿°')
                st.caption(f"ğŸ”¹ {dataset_name}/{split_name}", help=split_desc)

def render_all_datasets_markdown(ds_df):
    """
    ç”Ÿæˆæ‰€æœ‰æ•°æ®é›†çš„ markdown æ ¼å¼ä»‹ç»
    """
    md_lines = []
    for _, ds_row in ds_df.iterrows():
        dataset_name = ds_row['dataset_name']
        dataset_version = ds_row.get('dataset_version', '')
        description = ds_row.get('description', 'æ— æè¿°')
        paper_url = ds_row.get('paper_url') if len(str(ds_row.get('paper_url', ''))) else "æ— "
        dataset_url = ds_row.get('dataset_url') if len(str(ds_row.get('dataset_url', ''))) else "æ— "
        github_url = ds_row.get('github_url') if len(str(ds_row.get('github_url', ''))) else "æ— "
        splits = ds_row.get('dataset_splits', [])

        title_str = f"# {dataset_name}ï¼ˆv{dataset_version}ï¼‰" if dataset_version else f"# {dataset_name}"
        md_lines.append(title_str)
        md_lines.append("")
        # description
        md_lines.append(str(description))
        md_lines.append("")
        # ä¿¡æ¯æ¥æº
        md_lines.append("## ä¿¡æ¯æ¥æº")
        md_lines.append(f"* è®ºæ–‡: {paper_url}")
        md_lines.append(f"* æ•°æ®é›†: {dataset_url}")
        md_lines.append(f"* GitHub: {github_url}")
        md_lines.append("")
        # Split è¯´æ˜
        md_lines.append("## Split è¯´æ˜")
        if splits and isinstance(splits, (list, tuple)):
            for split in splits:
                split_name = split.get('display_name') or split.get('split_name') or 'æœªå‘½ååˆ‡åˆ†'
                split_desc = split.get('description', 'æ— æè¿°')
                md_lines.append(f"* **{split_name}**: {split_desc}")
        else:
            md_lines.append("* æ— åˆ‡åˆ†ä¿¡æ¯")
        md_lines.append("\n---\n")
    return "\n".join(md_lines)


def page1() -> None:

    initialze_sesstion_state()

    col_title, col_select, col_add_delete = st.columns([7, 2, 1], vertical_alignment="bottom")
    with col_title:
        st.title("Deep Research æ•°æ®ç»Ÿè®¡")
    with col_select:
        ds_df = to_dataset_rows(st.session_state["datasets_summary"])
        sp_df = to_split_rows(st.session_state["datasets_summary"])
        dataset_names = ds_df["dataset_name"].unique().tolist()
        selected_dataset = st.selectbox(
            "ğŸ“š é€‰æ‹©æ•°æ®é›†",
            options=["å…¨éƒ¨"] + sorted(dataset_names),
            index=0,
            key="dataset_selectbox",
            help="ğŸŒ¾ é€‰æ‹©å…·ä½“çš„æ•°æ®é›†ï¼Œèƒ½å¤Ÿçœ‹åˆ°æ•°æ®é›†çš„ç›¸å…³ä»‹ç»"
        )
    # st.write(sp_df)
    # åœ¨æ ‡é¢˜å’Œ selectbox ä¸‹æ–¹å±•ç¤ºæ•°æ®é›†æè¿°å¡ç‰‡
    if selected_dataset != "å…¨éƒ¨":
        ds_row = ds_df[ds_df["dataset_name"] == selected_dataset].iloc[0]
        render_dataset_description_card(ds_row)
    else:
        # ç”¨ st.expander å±•ç¤ºæ‰€æœ‰æ•°æ®é›†ç®€ä»‹
        with st.expander("ğŸ“ æ‰€æœ‰æ•°æ®é›†ç®€ä»‹", expanded=False):   
            for _, ds_row in ds_df.iterrows():
                dataset_name = ds_row['dataset_name']
                dataset_version = ds_row.get('dataset_version', '')
                description = ds_row.get('description', 'æ— æè¿°')
                paper_url = ds_row.get('paper_url') if len(str(ds_row.get('paper_url', ''))) else "æ— "
                dataset_url = ds_row.get('dataset_url') if len(str(ds_row.get('dataset_url', ''))) else "æ— "
                github_url = ds_row.get('github_url') if len(str(ds_row.get('github_url', ''))) else "æ— "
                splits = ds_row.get('dataset_splits', [])

                # ä¸€çº§æ ‡é¢˜
                st.markdown(f"# {dataset_name}ï¼ˆv{dataset_version}ï¼‰" if dataset_version else f"# {dataset_name}")
                # description
                st.markdown(description)
                # ä¿¡æ¯æ¥æº
                st.markdown("## ä¿¡æ¯æ¥æº")
                st.markdown(f"* è®ºæ–‡: {paper_url}")
                st.markdown(f"* æ•°æ®é›†: {dataset_url}")
                st.markdown(f"* GitHub: {github_url}")
                # Split è¯´æ˜
                st.markdown("## Split è¯´æ˜")
                if splits and isinstance(splits, (list, tuple)) and len(splits) > 0:
                    for split in splits:
                        split_name = split.get('display_name') or split.get('split_name') or 'æœªå‘½ååˆ‡åˆ†'
                        split_desc = split.get('description', 'æ— æè¿°').replace('\n', '\n\t')
                        st.markdown(f"* **{split_name}**: \n\t{split_desc}")
                else:
                    st.markdown("* æ— åˆ‡åˆ†ä¿¡æ¯")
                st.markdown("---")

            # ä¸‹è½½æŒ‰é’®
            all_md = render_all_datasets_markdown(ds_df)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰æ•°æ®é›†ç®€ä»‹ï¼ˆMarkdownï¼‰",
                data=all_md,
                file_name="datasets_summary.md",
                mime="text/markdown"
            )

    # ä¾§è¾¹æ ç­›é€‰å™¨
    st.sidebar.header("æ•°æ®ç­›é€‰")
    lang_opts = get_unique_values(ds_df, "languages")
    task_opts = get_unique_values(ds_df, "task_type")
    tool_opts = get_unique_values(ds_df, "tool_preference")
    org_opts = get_unique_values(ds_df, "organization")
    originality_opts = get_unique_values(sp_df, "originality")

    # sel_langs = st.sidebar.multiselect("ğŸŒ è¯­è¨€", options=lang_opts, default=[])
    sel_langs = []
    sel_tasks = st.sidebar.multiselect("ğŸ“ ä»»åŠ¡ç±»å‹", options=task_opts, default=[])
    sel_tools = st.sidebar.multiselect("ğŸ› ï¸ å·¥å…·åå¥½", options=tool_opts, default=[])
    # sel_orgs = st.sidebar.multiselect("ğŸ¢ ç»„ç»‡", options=org_opts, default=[])
    # sel_originality = st.sidebar.multiselect("âœ¨ åŸåˆ›æ€§", options=originality_opts, default=[])
    # ç»„ç»‡å’ŒåŸåˆ›æ€§ä½œä¸ºéšè—ç­›é€‰é¡¹ï¼Œä¸åœ¨ä¾§è¾¹æ æ˜¾ç¤ºï¼Œä½†ä¾ç„¶å‚ä¸åç»­ç­›é€‰é€»è¾‘
    sel_orgs = []
    sel_originality = []

    tri_state = {"å…¨éƒ¨": None, "æ˜¯": True, "å¦": False}
    sel_multimodal = tri_state[st.sidebar.selectbox("ğŸ–¼ï¸ å¤šæ¨¡æ€", options=list(tri_state.keys()), index=0)]
    sel_has_ref = tri_state[st.sidebar.selectbox("ğŸ“‘ æœ‰å‚è€ƒç­”æ¡ˆ", options=list(tri_state.keys()), index=0)]
    sel_has_reason = tri_state[st.sidebar.selectbox("ğŸ” æœ‰æ¨ç†è¿‡ç¨‹", options=list(tri_state.keys()), index=0)]

    view = st.sidebar.radio("ğŸ‘ï¸ è§†å›¾", options=["æŒ‰æ•°æ®é›†", "æŒ‰åˆ‡åˆ†"], index=0, horizontal=False)

    # ========== æ–°å¢æ•°æ®é›†æŒ‰é’®ä¸å¼¹çª— ==========
    with col_add_delete:
        col_add, col_delete = st.columns([1,1], gap= None)
        with col_add:
            if st.button("â•"):
                create_new_dataset(lang_opts, task_opts, tool_opts, org_opts, originality_opts)
        with col_delete:
            if st.button("â–"):
                delete_datasets_dialog()
            

    # åº”ç”¨ç­›é€‰
    filtered_sp = apply_filters(
        sp_df,
        languages=sel_langs,
        task_types=sel_tasks,
        tools=sel_tools,
        organizations=sel_orgs,
        originality=sel_originality,
        is_multimodal=sel_multimodal,
        has_reference_answer=sel_has_ref,
        has_reasoning_process=sel_has_reason,
    )

    # æ•°æ®é›†è§†å›¾ï¼šä»åˆ‡åˆ†ç­›é€‰ç»“æœæ¨å¯¼æ•°æ®é›†
    filtered_ds_names = set(filtered_sp["dataset_name"].unique().tolist())
    filtered_ds = ds_df[ds_df["dataset_name"].isin(filtered_ds_names)].copy()

    # å¦‚æœé€‰æ‹©äº†å…·ä½“æ•°æ®é›†ï¼Œåˆ™åªæ˜¾ç¤ºè¯¥æ•°æ®é›†ç›¸å…³æ•°æ®
    if selected_dataset != "å…¨éƒ¨":
        filtered_ds = filtered_ds[filtered_ds["dataset_name"] == selected_dataset]
        filtered_sp = filtered_sp[filtered_sp["dataset_name"] == selected_dataset]

    # æŒ‡æ ‡å¡ç‰‡
    render_metrics(filtered_ds, filtered_sp)

    # å›¾è¡¨ä¸è¡¨æ ¼
    if view == "æŒ‰æ•°æ®é›†":
        render_dataset_charts(filtered_ds, filtered_sp, animate=True)
        st.subheader("ğŸ“‹ æ•°æ®é›†æ˜ç»†")
        show_cols = [
            "dataset_name",
            "dataset_version",
            "organization",
            "task_type",
            "languages",
            "tool_preference",
            "num_total_samples",
            "update_time",
            "paper_url",
            "dataset_url",
            "github_url",
        ]
        st.dataframe(filtered_ds[show_cols], width='stretch', hide_index=True)
    else:
        render_split_charts(filtered_sp, animate=True)
        st.subheader("ğŸ“‹ åˆ‡åˆ†æ˜ç»†")
        show_cols = [
            "dataset_name",
            "display_name",
            "num_samples",
            "is_multimodal",
            "has_reference_answer",
            "has_reasoning_process",
            "originality",
            "file_path",
            "processed_samples",
            "correct_samples",
            "correct_samples_path",
        ]
        st.dataframe(filtered_sp[show_cols], width='stretch', hide_index=True)
        render_progress(filtered_sp)

    # st.sidebar.markdown("---")
    # st.sidebar.caption("è¿è¡Œï¼š`streamlit run streamlit_dashboard.py`")

    

    


if __name__ == "__main__":
    page1()
